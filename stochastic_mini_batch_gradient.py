# -*- coding: utf-8 -*-
"""stochastic mini-batch gradient

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-fks0143PPfTvuE3NHPtoVTmTWBFlyv8
"""

import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((100, 1)), X]

# Initialize parameters
theta_mbgd = np.random.randn(2, 1)
theta_sgd = np.random.randn(2, 1)

# Hyperparameters
learning_rate = 0.01
batch_size = 10
n_epochs = 100

# Lists to track cost values
costs_mbgd = []
costs_sgd = []

# Stochastic mini-batch gradient descent
for epoch in range(n_epochs):
    shuffled_indices = np.random.permutation(100)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]

    cost_mbgd = np.mean((X_b.dot(theta_mbgd) - y) ** 2)
    costs_mbgd.append(cost_mbgd)

    for i in range(0, 100, batch_size):
        X_batch = X_b_shuffled[i:i + batch_size]
        y_batch = y_shuffled[i:i + batch_size]

        gradient = -2 * X_batch.T.dot(y_batch - X_batch.dot(theta_mbgd))
        theta_mbgd -= learning_rate * gradient

# Regular stochastic gradient descent
for epoch in range(n_epochs):
    for i in range(100):
        random_index = np.random.randint(100)
        X_i = X_b[random_index:random_index + 1]
        y_i = y[random_index:random_index + 1]

        gradient = -2 * X_i.T.dot(y_i - X_i.dot(theta_sgd))
        theta_sgd -= learning_rate * gradient

    cost_sgd = np.mean((X_b.dot(theta_sgd) - y) ** 2)
    costs_sgd.append(cost_sgd)

# Plotting the cost values over epochs
plt.figure(figsize=(10, 6))
plt.plot(costs_mbgd, label='Mini-Batch GD')
plt.plot(costs_sgd, label='SGD')
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.title('Cost Function over Epochs')
plt.legend()
plt.grid()
plt.show()